{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from pettingzoo.utils.env import AECEnv\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import ultimate_tictactoe_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game env wrapper for MCTS search\n",
    "class State:\n",
    "\n",
    "    def __init__(self, env : AECEnv):\n",
    "        self.env = env\n",
    "\n",
    "    def gameEnded(self):\n",
    "        _, _, done, _ = self.env.last()\n",
    "        return done\n",
    "\n",
    "    def gameReward(self):\n",
    "        _, reward, _, _ = self.env.last()\n",
    "        return reward\n",
    "\n",
    "    def getActionMask(self):\n",
    "        observation, _, _, _ = self.env.last()\n",
    "        return observation[\"action_mask\"]\n",
    "\n",
    "    def getValidActions(self):\n",
    "        return np.flatnonzero(self.getActionMask())\n",
    "\n",
    "    def nextState(self, action):\n",
    "        new_env = deepcopy(self.env)\n",
    "        new_env.step(action)\n",
    "        player_changed = self.env.agent_selection != new_env.agent_selection\n",
    "        return State(new_env), player_changed\n",
    "\n",
    "    def getObservation(self):\n",
    "        return self.env.observe(self.currentAgent())[\"observation\"]\n",
    "\n",
    "    def currentAgent(self):\n",
    "        return self.env.agent_selection\n",
    "\n",
    "    def show(self, wait=False):\n",
    "        self.env.render()\n",
    "        if wait:\n",
    "            input(\"press any key to continue\")\n",
    "\n",
    "\n",
    "    def __eq__(self, x):\n",
    "        if not isinstance(x, State):\n",
    "            return False\n",
    "        # this should be enough\n",
    "        same_agent = self.env.agent_selection == x.env.agent_selection\n",
    "        observations_match = (self.getObservation() == x.getObservation()).all()\n",
    "        return same_agent and observations_match\n",
    "\n",
    "    def toStr(self):\n",
    "        o = self.getObservation()\n",
    "        # reduce dimensions from 3 to 2\n",
    "        o = np.sum(o, axis = 2) * (np.argmax(o, axis = 2) + 1)\n",
    "        return str(o)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.toStr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |     |     ||     |     |     ||     |     |     \n",
      "  -  |  -  |  -  ||  -  |  -  |  -  ||  -  |  -  |  -  \n",
      "_____|_____|_____||_____|_____|_____||_____|_____|_____\n",
      "     |     |     ||     |     |     ||     |     |     \n",
      "  -  |  -  |  -  ||  -  |  -  |  -  ||  -  |  -  |  -  \n",
      "_____|_____|_____||_____|_____|_____||_____|_____|_____\n",
      "     |     |     ||     |     |     ||     |     |     \n",
      "  -  |  -  |  -  ||  -  |  -  |  -  ||  -  |  -  |  -  \n",
      "=====|=====|=====||=====|=====|=====||=====|=====|=====\n",
      "     |     |     ||     |     |     ||     |     |     \n",
      "  -  |  -  |  -  ||  -  |  -  |  -  ||  -  |  -  |  -  \n",
      "_____|_____|_____||_____|_____|_____||_____|_____|_____\n",
      "     |     |     ||     |     |     ||     |     |     \n",
      "  -  |  -  |  -  ||  -  |  -  |  -  ||  -  |  -  |  -  \n",
      "_____|_____|_____||_____|_____|_____||_____|_____|_____\n",
      "     |     |     ||     |     |     ||     |     |     \n",
      "  -  |  -  |  -  ||  -  |  -  |  -  ||  -  |  -  |  -  \n",
      "=====|=====|=====||=====|=====|=====||=====|=====|=====\n",
      "     |     |     ||     |     |     ||     |     |     \n",
      "  -  |  -  |  -  ||  -  |  -  |  -  ||  -  |  -  |  -  \n",
      "_____|_____|_____||_____|_____|_____||_____|_____|_____\n",
      "     |     |     ||     |     |     ||     |     |     \n",
      "  -  |  -  |  -  ||  -  |  -  |  -  ||  -  |  -  |  -  \n",
      "_____|_____|_____||_____|_____|_____||_____|_____|_____\n",
      "     |     |     ||     |     |     ||     |     |     \n",
      "  -  |  -  |  -  ||  -  |  -  |  -  ||  -  |  -  |  -  \n",
      "     |     |     ||     |     |     ||     |     |     \n"
     ]
    }
   ],
   "source": [
    "env = ultimate_tictactoe_v1.env()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = State(env)\n",
    "state.getObservation().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def competition(p1, p2, num_games):\n",
    "    p1_win = 0\n",
    "    p2_win = 0\n",
    "    draw = 0\n",
    "    games = 0\n",
    "    while (games < num_games):\n",
    "        env.reset()\n",
    "        for agent in env.agent_iter():\n",
    "            if \"player_1\" in env.rewards:\n",
    "                if env.rewards[\"player_1\"] > 0:\n",
    "                    p1_win += 1\n",
    "                elif env.rewards[\"player_1\"] < 0:\n",
    "                    p2_win += 1\n",
    "            observation, reward, done, info = env.last()\n",
    "            action = None\n",
    "            env_pass = env.unwrapped\n",
    "            if agent == \"player_1\":\n",
    "                action = p1(observation, 1, env_pass) if not done else None\n",
    "            else:\n",
    "                action = p2(observation, 2, env_pass) if not done else None\n",
    "            env.step(action)\n",
    "            # env.render()\n",
    "            # print(\"\\n\") # this visualizes a single game\n",
    "        games += 1\n",
    "    draw = games - (p1_win + p2_win)\n",
    "    print(f\"p1_win: {p1_win / num_games}\")\n",
    "    print(f\"p2_win: {p2_win / num_games}\")\n",
    "    print(f\"draws: {draw / num_games}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_random(observation, agent, env):\n",
    "    action = random.choice(np.flatnonzero(observation['action_mask']))\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingExample:\n",
    "\n",
    "    def __init__(self, state, pi, reward):\n",
    "        self.state = state\n",
    "        self.pi = pi\n",
    "        self.reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-18 15:11:43.073125: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 9, 9, 2)]    0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 7, 7, 18)     342         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 5, 5, 18)     2934        ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 450)          0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " p (Dense)                      (None, 81)           36531       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " v (Dense)                      (None, 1)            451         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 40,258\n",
      "Trainable params: 40,258\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 430ms/step - loss: 1.0543 - p_loss: 0.0543 - v_loss: 1.0000\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0523 - p_loss: 0.0543 - v_loss: 0.9980\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0503 - p_loss: 0.0543 - v_loss: 0.9960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<neutralnet.NNet at 0x7ff364c41d30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neutralnet import NNet, apply_actionmask_to_policy\n",
    "\n",
    "nnet = NNet(81)\n",
    "examples = [TrainingExample(state, np.full(81, 1.0 / 81), 1) for _ in range(32)]\n",
    "nnet.train(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81,)\n",
      "0.0029999004\n"
     ]
    }
   ],
   "source": [
    "policy, value = nnet.predict(state)\n",
    "print(policy.shape)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSNode:\n",
    "\n",
    "    def __init__ (self, p, q):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        p : policy in this state\n",
    "        q : q value of this state\n",
    "        \"\"\"\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        # n[a] : number of times and action has been performed from this state\n",
    "        self.n = np.zeros(len(p))\n",
    "        # q_a : q values of states following performing an action a\n",
    "        self.q_a = np.zeros(len(p))\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "\n",
    "    def __init__(self, nnet, num_mcts_sims, max_depth = 10):\n",
    "        self.nnet = nnet\n",
    "        self.nodes = {}\n",
    "        self.c_puct = 1.0\n",
    "        self.num_mcts_sims = num_mcts_sims\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def search(self, s):\n",
    "        for _ in range(self.num_mcts_sims):\n",
    "            self._search(s, self.max_depth)\n",
    "\n",
    "    def _search(self, s, max_depth):\n",
    "        if s.gameEnded(): return s.gameReward()\n",
    "\n",
    "        if s not in self.nodes:\n",
    "            p, v = self.nnet.predict(s)\n",
    "            self.nodes[s] = MCTSNode(p, v)\n",
    "            return v\n",
    "\n",
    "        node = self.nodes[s]\n",
    "\n",
    "        if max_depth == 0:\n",
    "            # max depth reached, returning a heuristic value of this state\n",
    "            return node.q\n",
    "      \n",
    "        # upper confidence bound\n",
    "        ucb = node.q_a + self.c_puct * node.p * np.sqrt(np.sum(node.n)) / (1 + node.n)\n",
    "        ucb[s.getActionMask() == 0] = -np.inf\n",
    "        # choose best action based on ucb\n",
    "        a = np.argmax(ucb)\n",
    "        \n",
    "        sp, player_changed = s.nextState(a)\n",
    "        v = self._search(sp, max_depth - 1)\n",
    "        if player_changed:\n",
    "            v = -v\n",
    "\n",
    "        node.q_a[a] = (node.n[a] * node.q_a[a] + v) / (node.n[a] + 1)\n",
    "        node.n[a] += 1\n",
    "        return v\n",
    "\n",
    "    # improved policy\n",
    "    def pi(self, s : State):\n",
    "        node = self.nodes[s]\n",
    "        n_sum = np.sum(node.n)\n",
    "        if n_sum == 0:\n",
    "            return node.p\n",
    "\n",
    "        return node.n / n_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcts = MCTS(nnet, 2)\n",
    "mcts.search(state)\n",
    "mcts.pi(state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    \n",
    "    def predict(self, state : State):\n",
    "        p = np.random.uniform(81)\n",
    "        return apply_actionmask_to_policy(p, state.getActionMask()), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pit(new_nnet : NNet, nnet : NNet, games_played = 40):\n",
    "    new_nnet_tag = \"player_1\"\n",
    "    nnet_tag = \"player_2\"\n",
    "    wins = 0\n",
    "    ties = 0\n",
    "\n",
    "    for g in range(games_played):\n",
    "        env = ultimate_tictactoe_v1.env()\n",
    "        env.reset()\n",
    "        s = State(env)\n",
    "        # swap players before each round\n",
    "        new_nnet_tag, nnet_tag = nnet_tag, new_nnet_tag  \n",
    "        agents = {new_nnet_tag : new_nnet, nnet_tag : nnet}\n",
    "\n",
    "        while not s.gameEnded():\n",
    "            agent = agents[s.currentAgent()]\n",
    "            p, _ = agent.predict(s)\n",
    "            action = np.random.choice(len(p), p=p)\n",
    "            s.env.step(action)\n",
    "\n",
    "        if s.gameReward() == 0:\n",
    "            ties += 1\n",
    "       \n",
    "        if s.gameReward() == 1 and s.currentAgent() == new_nnet_tag:\n",
    "            wins += 1\n",
    "    \n",
    "        if s.gameReward() == -1 and s.currentAgent() != new_nnet_tag:\n",
    "            wins += 1\n",
    "        \n",
    "            \n",
    "    frac_win = wins / (games_played - ties)\n",
    "    return frac_win\n",
    "\n",
    "# training\n",
    "def policyIterSP(env : AECEnv, num_iters = 10, num_eps = 10,  num_mcts_sims=25, frac_win_thresh = 0.55):\n",
    "    # hard coded action space size\n",
    "    nnet = NNet(81)\n",
    "    frac_win = pit(nnet, RandomPlayer())                              # compare new net with a random player\n",
    "    print(\"frac_wins against a random player\", frac_win)\n",
    "    examples = []\n",
    "    for i in range(num_iters):\n",
    "        for e in range(num_eps):\n",
    "            examples += executeSelfPlayEpisode(env, nnet, num_mcts_sims)    # collect examples from this game\n",
    "            print(\"episode done\")\n",
    "        new_nnet = nnet.train(examples)\n",
    "        frac_win = pit(new_nnet, nnet)                                # compare new net with previous net\n",
    "        print(\"frac_win\", frac_win)\n",
    "        if frac_win > frac_win_thresh:\n",
    "            print(\"new net is better!\")\n",
    "            nnet = new_nnet                                           # replace with new net\n",
    "            frac_win = pit(nnet, RandomPlayer())                      # compare new net with a random player\n",
    "            print(\"frac_wins against a random player\", frac_win)\n",
    "        examples = random.sample(examples, len(examples) // 2)        # discard half of the examples\n",
    "    return nnet\n",
    "\n",
    "def executeSelfPlayEpisode(env : AECEnv, nnet, num_mcts_sims = 3):\n",
    "    examples = []\n",
    "    env.reset()\n",
    "    s = State(env)\n",
    "    # s.show(wait = False)\n",
    "    mcts = MCTS(nnet, num_mcts_sims)\n",
    "\n",
    "    while True:\n",
    "        mcts.search(s)\n",
    "        pi = mcts.pi(s)\n",
    "        examples.append(TrainingExample(deepcopy(s), pi, None))  # rewards can not be determined yet\n",
    "        a = np.random.choice(len(pi), p=pi)                      # sample action from improved policy\n",
    "        s, _ = s.nextState(a)\n",
    "        # s.show(wait = False)\n",
    "        if s.gameEnded():\n",
    "            examples = assignRewards(examples, s.gameReward(), s.currentAgent())\n",
    "            return examples\n",
    "\n",
    "def assignRewards(examples, reward, player_w_reward):\n",
    "    for e in examples:\n",
    "        e.reward = reward if e.state.currentAgent() == player_w_reward else -reward\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 9, 9, 2)]    0           []                               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 7, 7, 18)     342         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 5, 5, 18)     2934        ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 450)          0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " p (Dense)                      (None, 81)           36531       ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " v (Dense)                      (None, 1)            451         ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 40,258\n",
      "Trainable params: 40,258\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "frac_wins against a random player 0.41935483870967744\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "Epoch 1/3\n",
      "18/18 [==============================] - 1s 3ms/step - loss: 0.7293 - p_loss: 0.0541 - v_loss: 0.6752\n",
      "Epoch 2/3\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5229 - p_loss: 0.0535 - v_loss: 0.4693\n",
      "Epoch 3/3\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.3844 - p_loss: 0.0531 - v_loss: 0.3312\n",
      "frac_win 0.43333333333333335\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "Epoch 1/3\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6866 - p_loss: 0.0537 - v_loss: 0.6329\n",
      "Epoch 2/3\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3702 - p_loss: 0.0530 - v_loss: 0.3173\n",
      "Epoch 3/3\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2666 - p_loss: 0.0524 - v_loss: 0.2142\n",
      "frac_win 0.42857142857142855\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "Epoch 1/3\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5323 - p_loss: 0.0532 - v_loss: 0.4791\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.3103 - p_loss: 0.0527 - v_loss: 0.2576\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2544 - p_loss: 0.0522 - v_loss: 0.2022\n",
      "frac_win 0.5161290322580645\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "Epoch 1/3\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4434 - p_loss: 0.0526 - v_loss: 0.3908\n",
      "Epoch 2/3\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2881 - p_loss: 0.0521 - v_loss: 0.2360\n",
      "Epoch 3/3\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2476 - p_loss: 0.0516 - v_loss: 0.1960\n",
      "frac_win 0.5\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "Epoch 1/3\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5368 - p_loss: 0.0521 - v_loss: 0.4847\n",
      "Epoch 2/3\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.3189 - p_loss: 0.0515 - v_loss: 0.2673\n",
      "Epoch 3/3\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2662 - p_loss: 0.0510 - v_loss: 0.2152\n",
      "frac_win 0.4857142857142857\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "Epoch 1/3\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4712 - p_loss: 0.0519 - v_loss: 0.4193\n",
      "Epoch 2/3\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.3122 - p_loss: 0.0514 - v_loss: 0.2608\n",
      "Epoch 3/3\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2713 - p_loss: 0.0507 - v_loss: 0.2206\n",
      "frac_win 0.5483870967741935\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "Epoch 1/3\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3230 - p_loss: 0.0514 - v_loss: 0.2717\n",
      "Epoch 2/3\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2463 - p_loss: 0.0507 - v_loss: 0.1956\n",
      "Epoch 3/3\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2263 - p_loss: 0.0500 - v_loss: 0.1763\n",
      "frac_win 0.3225806451612903\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "Epoch 1/3\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.4014 - p_loss: 0.0505 - v_loss: 0.3509\n",
      "Epoch 2/3\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.2848 - p_loss: 0.0497 - v_loss: 0.2351\n",
      "Epoch 3/3\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2434 - p_loss: 0.0490 - v_loss: 0.1945\n",
      "frac_win 0.5666666666666667\n",
      "new net is better!\n",
      "frac_wins against a random player 0.6666666666666666\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "Epoch 1/3\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.4260 - p_loss: 0.0498 - v_loss: 0.3762\n",
      "Epoch 2/3\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.2815 - p_loss: 0.0489 - v_loss: 0.2326\n",
      "Epoch 3/3\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2361 - p_loss: 0.0480 - v_loss: 0.1881\n",
      "frac_win 0.4482758620689655\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "Epoch 1/3\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.4249 - p_loss: 0.0489 - v_loss: 0.3760\n",
      "Epoch 2/3\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.2764 - p_loss: 0.0481 - v_loss: 0.2283\n",
      "Epoch 3/3\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2411 - p_loss: 0.0471 - v_loss: 0.1940\n",
      "frac_win 0.5806451612903226\n",
      "new net is better!\n",
      "frac_wins against a random player 0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "env = ultimate_tictactoe_v1.env()\n",
    "nnet = policyIterSP(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monte_carlo\n",
    "import copy\n",
    "\n",
    "def p_monte(observation, agent, env):\n",
    "    def monte_carlo_move(game_board, player):\n",
    "        # copy game board so MC can do it's game tree search on it without affecting actual board\n",
    "        game_board2 = copy.deepcopy(game_board)\n",
    "        root = MCTSNode(state=game_board2, player_number=player, origin=player)\n",
    "        root.want_to_win = player\n",
    "        mcts_move = root.best_action()\n",
    "        return mcts_move\n",
    "    action = monte_carlo_move(env.board, agent)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_neural(observation, agent, env):\n",
    "    def decide(nn, observation):\n",
    "        x = observation[\"observation\"]\n",
    "        x = np.expand_dims(x, 0)\n",
    "        p, v = nn.nnet.predict(x)\n",
    "        p = p[0]\n",
    "        p = apply_actionmask_to_policy(p, observation[\"action_mask\"])\n",
    "        return p, v[0][0]\n",
    "    p, _ = decide(nnet, observation)\n",
    "    action = np.random.choice(len(p), p=p)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1_win: 0.39\n",
      "p2_win: 0.31\n",
      "draws: 0.3\n"
     ]
    }
   ],
   "source": [
    "# Neural network vs Random\n",
    "competition(p_random, p_neural, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OrderEnforcingWrapper' object has no attribute 'board'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/93/r9jshc2979z1l55rb4p565nc0000gn/T/ipykernel_60055/3379347418.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompetition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_monte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/93/r9jshc2979z1l55rb4p565nc0000gn/T/ipykernel_60055/1461361300.py\u001b[0m in \u001b[0;36mcompetition\u001b[0;34m(p1, p2, num_games)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"player_1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/93/r9jshc2979z1l55rb4p565nc0000gn/T/ipykernel_60055/2524101201.py\u001b[0m in \u001b[0;36mp_monte\u001b[0;34m(observation, agent, env)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmcts_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmcts_move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonte_carlo_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/tictacbro/venv/lib/python3.8/site-packages/pettingzoo/utils/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{value} cannot be accessed before reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{value}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OrderEnforcingWrapper' object has no attribute 'board'"
     ]
    }
   ],
   "source": [
    "competition(p_monte, p_random, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/neural/assets\n"
     ]
    }
   ],
   "source": [
    "nnet.nnet.save(\"saved_model/neural\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c7c38661b9d677a52a5c0789c23abb74854ab3b06d3a2bffcd4c9f8936097f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
